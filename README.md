# hotel bookings etl

Repo created to store source code of data engineering project involving business process definition, data modeling, artificial data generation, data seeding, data streaming, data processing and automated testing

## Business process
Below image depicts business process defined prior to identifying data required for project
![image](https://github.com/ppkgtmm/hotel-bookings-etl/assets/57994731/97fd9e49-0245-4c5c-8aae-4c9a35cfbd6e)


A few constraints need to be enforced for simplicity and ease of data management, such constraints are
- Each user must also be registered as guest before making reservation for ownself
- A guest cannot have multiple bookings with overlapping dates
- None of the guests can be tied to multiple rooms reserved through same booking
- The following cannot be done when less than 7 days left before checkin date : room booking, amenity purchase, cancelation or modification of booking and amenities purchased 

## Data modeling

## Usage

Make sure to be inside project directory in your terminal

#### Initialization

Required only for the first time you are running the project

1. Copy file `example.env` into a new file called `.env` in the same directory
   
2. In `.env` file created, fill in values that have following pattern `<TO BE FILLED>` as needed 

3. Run following to allow helper script execution

```
chmod +x run.sh
```

#### Set up

With this step, virtual environment is created (if not exist) and libraries are installed. Also, containers required to execute the project are started. Eventually, transactional and analytical databases are initialized

```
./run.sh setup
```

#### Data generation

Fake data related to hotel reservations e.g. users, rooms, bookings, amenities etc. are generated by running the command below

```
./run.sh datagen
```

#### Data population

After running commend below, data generated in previous step is populated to transactional database

```
./run.sh seed
```

#### ETL - Part 1

In this step, insert, update or delete operations inside transactional database is captured by connector from database log file and sent to kafka broker for further consumption by stream processer. Furthermore, some dimesions namely location and date dimesnions are populated

```
./run.sh etl
```


- Every change in dimension related tables are captured while only latest state of fact related tables are retained in staging area
- If the command fails because of `AssertionError` or `connection issue`, retry after some time

#### ETL - Part 2 

1. Run command below to start airflow scheduler and webserver
```
./run.sh airflow
```

2. After a few minutes, go to `http://localhost:8080/` in web browser then login to airflow website with `AIRFLOW_ADMIN_USERNAME` and `AIRFLOW_ADMIN_PASSWORD` defined in `.env` file
   
3. Turn on the following pipelines or dags
   - clean_up
   - process_facts

4. Optionally, click on any dag name to monitor it while running

Upon successful completion of the pipeliness, fact tables will be populated and staging area will be cleaned up to reduce storage consumption

#### Run tests
```
./run.sh test
```
Firstly, test fact data yet to be processed i.e. bookings or amenities with at least 7 days different from current date are populated in staging tables. Then, airflow dag which processes facts

#### Tear down

With this step, containers in use by project are stopped and removed

```
./run.sh down
```
