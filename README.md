# hotel bookings etl

Make sure to be inside project directory in your terminal

**Initialization**

Required only for the first time you are running the project

1. Copy file `example.env` into a new file called `.env` in the same directory
   
2. In `.env` file created, fill in values that have following pattern `<TO BE FILLED>` as needed 

3. Run following to allow helper script execution

```
chmod +x run.sh
```
<br />

**Set up**

With this step, virtual environment is created (if not exist) and libraries are installed. Also, containers required to execute the project are started. Eventually, transactional database and data warehouse are initialized

```
./run.sh setup
```
<br />

**Data generation**

Fake data related to hotel reservations e.g. users, rooms, bookings, amenities etc. are generated by running the command below

```
./run.sh datagen
```
<br />

**Data population**

After running commend below, data generated in previous step is populated to transactional database

```
./run.sh seed
```
<br />

**Streaming ETL**

In this step, insert, update or delete operations inside transactional database is captured from database log file by connector and sent to kafka broker for further consumption by stream processer. Furthermore, some dimesions namely location and date dimesnions are populated

```
./run.sh etl
```


- Every change in dimension related tables are captured while only latest state of fact related tables are retained in staging tables
- If the command fails because of `AssertionError` or `connection issue`, retry after some time
<br />

**Batch Loading**

1. Run command below to start airflow scheduler and webserver
```
./run.sh airflow
```

2. After a few minutes, go to `http://localhost:8080/` in web browser then login to airflow website with `AIRFLOW_ADMIN_USERNAME` and `AIRFLOW_ADMIN_PASSWORD` defined in `.env` file
   
3. Turn on the following pipelines or dags
- clean_up
- process_facts
- process_full_picture

4. Optionally, click on any dag name to monitor it while running

Upon successful completion of the pipelines, fact tables and full picture table will be populated. In addition, staging tables will also be cleaned up to reduce storage consumption

<br />

**Testing**

Fact tables population logic are tested in this step to verify if bookings or amenities data are correctly inserted and if the data are inserted only when less than 7 days is left prior to checkin date

```
./run.sh test
```

Test run is considered as failed when  `AssertionError` is thrown

<br />

**Tear down**

With this step, containers in use by project are stopped and removed

```
./run.sh down
```
